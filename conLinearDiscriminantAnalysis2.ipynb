{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "158684d1",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0ad4d4",
   "metadata": {},
   "source": [
    "### Explore data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c3dfbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "import math\n",
    "import scipy  \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "from plotly.offline import plot## Exploratory Data Analysis\n",
    "\n",
    "#import warnings\n",
    "#warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8583e47",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e04c7b",
   "metadata": {},
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57a6204e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_Dataset(y):\n",
    "    df = pd.read_csv(y)\n",
    "    print(\"The dataset: \"+y+ \" has {} credit record\".format(len(df)))\n",
    "    training_data, testing_data = train_test_split(df, test_size=0.2, random_state=44)    \n",
    "    return df,training_data, testing_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7bd101",
   "metadata": {},
   "source": [
    "### Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be8331dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualise(df):\n",
    "    fig = make_subplots(rows=1, cols=2, specs=[[{\"type\": \"pie\"}, {\"type\": \"bar\"}]])\n",
    "    colors = ['pink', 'skyblue'] \n",
    "    fig.add_trace(go.Pie(labels=df[df.columns[-1]].value_counts().index,\n",
    "                                 values=df[df.columns[-1]].value_counts().values), 1, 1)\n",
    "\n",
    "    fig.update_traces(hoverinfo='label+percent', textfont_size=20,\n",
    "                      marker=dict(colors=colors, line=dict(color='#000000', width=2)))\n",
    "\n",
    "    fig.add_trace(go.Bar(x=df[df.columns[-1]].value_counts().index, y=df[df.columns[-1]].value_counts().values, marker_color = colors), 1,2)\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291f2d7b",
   "metadata": {},
   "source": [
    "## Model developement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3bc5dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(df):\n",
    "    valuecounts=df[df.columns[-1]].value_counts().index\n",
    "    majlabel=valuecounts[0]\n",
    "    minlabel=valuecounts[1:]\n",
    "    if len(minlabel)==1:\n",
    "        minlabel=int(minlabel[0])\n",
    "    return minlabel,int(majlabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7efac7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def findNeighbours(maj,df):\n",
    "    #find neighbours for each instance in the whole dataset \n",
    "    neigh = NearestNeighbors(n_neighbors=6)\n",
    "    neigh.fit(df)\n",
    "   # NearestNeighbors(n_neighbors=5) change it to six \n",
    "    NNAllDatasetar = neigh.kneighbors(df, return_distance=False)\n",
    "    NNAllDataset = pd.DataFrame(NNAllDatasetar, columns = ['index','n1','n2','n3','n4','n5'])\n",
    "    \n",
    "    #find the subset of the instances that belong to the majority class\n",
    "    NNMAJ = NNAllDataset[NNAllDataset.index.isin(maj.index)]\n",
    "    NNMAJ = NNMAJ.drop(['index'], axis=1)\n",
    "    NNMAJ =pd.merge(maj, NNMAJ, left_index=True, right_index=True)\n",
    "    return(NNMAJ)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "582241b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateweight(df,maj):\n",
    "    #  add  a column in which for each neighbooring point by 1 if it belongs to the majority class and 0 if it belongs to the minority class\n",
    "    df.insert(len(df.columns) ,\"n1W\", np.where(df[\"n1\"].isin(maj.index) , 1, 0))\n",
    "    df.insert(len(df.columns) ,\"n2W\", np.where(df[\"n2\"].isin(maj.index) , 1, 0))\n",
    "    df.insert(len(df.columns) ,\"n3W\", np.where(df[\"n3\"].isin(maj.index) , 1, 0))\n",
    "    df.insert(len(df.columns) ,\"n4W\", np.where(df[\"n4\"].isin(maj.index) , 1, 0))\n",
    "    df.insert(len(df.columns) ,\"n5W\", np.where(df[\"n5\"].isin(maj.index) , 1, 0))\n",
    "    df\n",
    "    \n",
    "    #  Calculate the weight \n",
    "    df.insert( len(df.columns) ,\"weight\", df[['n1W', 'n2W','n3W', 'n4W', 'n5W']].mean(axis=1),True)\n",
    "    \n",
    "    #  Sort the dataframe by the weight \n",
    "    df=df.sort_values(by=['weight'] ,ascending=False)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58eaf8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def repPoints(maj,df,n):\n",
    " #   resultfinal= df.head(n)\n",
    "  #  return maj[maj.index.isin(resultfinal.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac0336a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate(df,minlabel, majlabel):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    P = (df[df.columns[-1]] == minlabel).sum() #Number of instances in the minority class\n",
    "    N = (df[df.columns[-1]] == majlabel).sum() #Number of instances in the majority class\n",
    "   \n",
    "    n1 = P**2/N\n",
    "    \n",
    "    majclass = df[df[df.columns[-1]] == majlabel]\n",
    "    sigma = np.var(majclass.to_numpy()) #Strandard deviation of the majority class  # square root variance formula\n",
    "    Zalpha = scipy.stats.norm.ppf(.05) #the critical value of the Z test at the significance level α\n",
    "    epsilone =  pow(10,-4) # acceptable tolerance error that can be adjusted as required 10 power -4\n",
    "    \n",
    "    n2= (N*Zalpha*epsilone*sigma)/((N*epsilone**2)+ (Zalpha*epsilone*sigma**2))\n",
    "    \n",
    "    pr = n2/n1\n",
    "    \n",
    "    M = 1.5\n",
    "    \n",
    "    if pr < 1:\n",
    "         size = n1\n",
    "    elif pr > M:\n",
    "         size = n1*M\n",
    "    else:\n",
    "         size = n2\n",
    "    return size\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94cd8079",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatinate(df,ds):\n",
    "    ds1 = df \n",
    "    ds1 = ds1.iloc[: , :-1]\n",
    "    ds2 = ds\n",
    "   # ds1 = ds1.drop(ds1.index[0]) #if the same instance can't be concatinated with itself \n",
    "    \n",
    "    mainds= pd.DataFrame()\n",
    "    m=len(ds2.index)\n",
    "    for p in range(m):\n",
    "        mainds = mainds.append((ds1.assign(key=1).merge(ds2.head(1).assign(key=1), on='key').drop('key',axis=1)), ignore_index=True)\n",
    "        ds2 = ds2.iloc[1: , :] \n",
    "    return (mainds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c22b13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#figure out the subset\n",
    "def representeticepointssel(df,c):\n",
    "    if df.size<=c: #number of rows\n",
    "        tmpSet = df\n",
    "    else:\n",
    "        tmpSet = []\n",
    "        vecB= np.mean(df.to_numpy())\n",
    "        for i in range(c):\n",
    "            maxDist = 0\n",
    "            for p in df.iterrows():  # iterate it by rows not columns\n",
    "                if i==0:\n",
    "                    minDist = scipy.spatial.distance.pdist((p, vecB),'euclidean') #vecA is p vecB mean value of all the rows in df\n",
    "                else:\n",
    "                    # for a given p, if p's min distance to any q in tmpset is biggest, then p is next representative point \n",
    "                    minDist = np.min([scipy.spatial.distance.pdist((p, q),'euclidean') for q in tmpSet])\n",
    "                if minDist >= maxDist:\n",
    "                    maxPoint = p\n",
    "                    maxDist = minDist\n",
    "            tmpSet.append(maxPoint)\n",
    "        return(tmpSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d18a1a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concattest(df,finalMaj):\n",
    "    ds1 = df.iloc[: , :-1]\n",
    "    df = pd.concat([ds1, df], axis=1)\n",
    "    df.columns = list(finalMaj.columns)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9619889a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def repPoints(neighboursds,df,n):\n",
    "\n",
    "    rep1 = neighboursds[neighboursds[\"weight\"] > neighboursds.head(n)[\"weight\"].iloc[-1]]\n",
    "    rep2 = neighboursds[neighboursds[\"weight\"] == neighboursds.head(n)[\"weight\"].iloc[-1]]\n",
    "\n",
    "    rep1 =  df[df.index.isin(rep1.index)]\n",
    "    rep2 =  df[df.index.isin(rep2.index)]\n",
    "\n",
    "    if  (neighboursds.head(n)[\"weight\"].iloc[-1] == neighboursds.head(n+1)[\"weight\"].iloc[-1]):\n",
    "        if (len(rep1.index)>0):\n",
    "            m=n-len(rep1.index)\n",
    "            rep2= df.head(m) #representeticepointssel(rep2,m)\n",
    "            resultfinal= rep1.append(rep2)\n",
    "        else :\n",
    "            rep2= df.head(n) #representeticepointssel(rep2,n)\n",
    "            resultfinal= rep1.append(rep2)   \n",
    "\n",
    "    else :\n",
    "        resultfinal= df.head(n) \n",
    "    return resultfinal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7f7939dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def con(dftrain,dftest,df):\n",
    "    minlabel, majlabel= get_labels(dftrain)\n",
    "    majds = dftrain[dftrain[dftrain.columns[-1]] == majlabel]\n",
    "    minds = dftrain[dftrain[dftrain.columns[-1]] == minlabel]\n",
    "    \n",
    "\n",
    "    IRO= len(majds.index)/len(minds.index)\n",
    "\n",
    "    neighboursds = findNeighbours(majds,df)\n",
    "    neighboursds = calculateweight(neighboursds,majds)\n",
    "\n",
    "    size = calculate(dftrain,minlabel, majlabel)\n",
    "\n",
    "    reppoints=repPoints(neighboursds,majds,math.ceil(size)) # use math.ceil(size)  or math math.floor(size)\n",
    "\n",
    "\n",
    "    finalMaj=concatinate(majds,reppoints)\n",
    "    finalMin=concatinate(minds,minds) \n",
    "\n",
    "    IRcon= len(finalMaj.index)/len(finalMin.index)\n",
    "\n",
    "    finaldf=pd.concat([finalMaj,finalMin])\n",
    "    finaltest =concattest(dftest,finalMaj)   \n",
    "\n",
    "    #compare.loc[len(compare.index)] = [dataset, len(dftrain.index), len(majds.index),len(minds.index),IRO,len(reppoints.index),len(finaldf.index),len(finalMaj.index),len(finalMin.index),IRcon] \n",
    "    return finaldf, finaltest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d076e057",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "datasets=[\"abalone9-18.csv\",\"Breast.csv\",\"ecoli-0-1_vs_2-3-5.csv\",\"ecoli-0-1_vs_5.csv\",\"ecoli-0-1-4-7_vs_5-6.csv\",\n",
    "\"ecoli-0-2-3-4_vs_5.csv\",\"ecoli-0-4-6_vs_5.csv\",\"ecoli-0-6-7_vs_5.csv\",\"ecoli2.csv\",\"ecoli3.csv\", \"glass0123vs456.csv\",\n",
    "\"glass0.csv\",\"glass1.csv\",\"glass6.csv\",\"haberman.csv\",\"iris1.csv\",\"leaf.csv\",\"new-thyroid1.csv\",\"new-thyroid2.csv\",\n",
    "\"parkinsons.csv\",\"seeds.csv\",\"spect.csv\",\"wpbc.csv\",\"yeast-1_vs_7.csv\",\"yeast-2_vs_4.csv\",\n",
    "\"abalone11–17.csv\",\"abalone4–8.csv\",\"abalone5–10.csv\",\"BreastCancerWisconsin.csv\",\"ecoli1.csv\",\"eligibility-loan.csv\",\n",
    "\"iris0.csv\",\"lung-cancer.csv\",\"Maternal-Risk-lmvsh.csv\",\"page-blocks1vs2345.csv\",\"page-blocks2vs4.csv\",\"page-blocks3vs5.csv\",\n",
    "\"pima-indians-diabetes.csv\",\"pima.csv\",\"wheat1.csv\",\"wilt.csv\",\n",
    "\"winequality-red-3456vs78.csv\",\"winequality-red-34vs56.csv\",\"wisconsin.csv\",\"yeast1.csv\",\"yeast3.csv\"]\n",
    "k=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0b427a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scores(y_test,predictions):\n",
    "    AUC=0\n",
    "    accuracy= round(accuracy_score(y_test, predictions)*100, 2)\n",
    "    F1 = round(f1_score(y_test, predictions, average='weighted')*100, 2)\n",
    "    precision = round(precision_score(y_test, predictions, average='weighted')*100, 2)\n",
    "    recall = round(recall_score(y_test, predictions, average='weighted')*100, 2)\n",
    "    try:\n",
    "        AUC = round(roc_auc_score(y_test, predictions, average='weighted')*100, 2)\n",
    "    except ValueError:\n",
    "        pass\n",
    "    return accuracy,F1,precision,recall,AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e96bb235",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epochsScore(dataset,k):\n",
    "\n",
    "    path=\"splitdatasets/\"+dataset+\"/\"+dataset\n",
    "    compare = pd.DataFrame(columns=['iteration','accuracy','F1','precision',\"recall\",\"AUC\"])\n",
    "\n",
    "    for x in range(k):\n",
    "        train=pd.read_csv(path+'_train_'+str(x)+'.csv',sep='\\t', index_col=0)\n",
    "        test=pd.read_csv(path+'_test_'+str(x)+'.csv',sep='\\t', index_col=0)\n",
    "        df = pd.read_csv(\"datasets/\"+dataset)\n",
    "        train,test= con(train,test,df)\n",
    "        \n",
    "        lda_con = LinearDiscriminantAnalysis()        \n",
    "        X_train_con = train.drop(train.columns[-1], axis=1)\n",
    "        X_test_con = test.drop(test.columns[-1], axis=1)\n",
    "        \n",
    "\n",
    "        y_train_con = train[train.columns[-1]]\n",
    "        y_test_con = test[test.columns[-1]]\n",
    "        \n",
    "        lda_con.fit(X_train_con, y_train_con)\n",
    "        predictions_con = lda_con.predict(X_test_con)\n",
    "        accuracy,F1,precision,recall,AUC =  scores(y_test_con,predictions_con)\n",
    "        compare.loc[len(compare.index)]=[int(x), accuracy,F1,precision,recall,AUC]\n",
    "    avacc = compare[\"accuracy\"].mean()\n",
    "    avF1 = compare[\"F1\"].mean()\n",
    "    avPrecision = compare[\"precision\"].mean()\n",
    "    avrecall = compare[\"recall\"].mean()\n",
    "    avAUC = compare[\"AUC\"].mean()\n",
    "    compare.loc[len(compare.index)]=[\"average\", avacc,avF1,avPrecision,avrecall,avAUC]\n",
    "\n",
    "    return avacc,avF1,avPrecision,avrecall,avAUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "71b39fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def complda(datasets):\n",
    "    classificationCompare = pd.DataFrame(columns=['Dataset','Version','Accuracy score %', \"F1 Score %\",'precision score %', \"recall Score %\", \"AUC Score %\"])\n",
    "   # compare = pd.DataFrame(columns=['Dataset','original count','original Majority','original Minority',\" Imbalance ratio original\",'representitive points','con count','con Majority','con Minority',\" Imbalance ratio con\"])\n",
    "\n",
    "    for dataset in datasets:\n",
    "        #classification on original dataset\n",
    "        df = pd.read_csv(\"datasets/\"+dataset)\n",
    "        print(dataset)\n",
    "        X = df.drop(df.columns[-1], axis=1)\n",
    "        y = df[df.columns[-1]]\n",
    "        \n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=44)\n",
    "        lda_or = LinearDiscriminantAnalysis()\n",
    "        lda_or.fit(X_train, y_train)\n",
    "        \n",
    "        predictions_or = lda_or.predict(X_test)\n",
    "        \n",
    "        accuracy,F1,precision,recall,AUC =  scores(y_test,predictions_or)\n",
    "        classificationCompare.loc[len(classificationCompare.index)]=[dataset,\"original\", accuracy,F1,precision,recall,AUC]\n",
    "        #classification on concatinated dataset\n",
    "        \n",
    "        accuracy,F1,precision,recall,AUC =  epochsScore(dataset,5)\n",
    "        classificationCompare.loc[len(classificationCompare.index)]=[dataset,\"concatinated\", accuracy,F1,precision,recall,AUC]\n",
    "\n",
    "    return classificationCompare "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "38e508b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abalone9-18.csv\n",
      "42\n",
      "42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\donia\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning:\n",
      "\n",
      "Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n",
      "42\n",
      "42\n",
      "Breast.csv\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "ecoli-0-1_vs_2-3-5.csv\n",
      "24\n",
      "24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\donia\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning:\n",
      "\n",
      "Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "\n",
      "C:\\Users\\donia\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning:\n",
      "\n",
      "Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n",
      "24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\donia\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning:\n",
      "\n",
      "Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "('Lengths must match to compare', (244,), (0,))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [55], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m classificationCompare \u001b[38;5;241m=\u001b[39m \u001b[43mcomplda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdatasets\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn [54], line 23\u001b[0m, in \u001b[0;36mcomplda\u001b[1;34m(datasets)\u001b[0m\n\u001b[0;32m     20\u001b[0m     classificationCompare\u001b[38;5;241m.\u001b[39mloc[\u001b[38;5;28mlen\u001b[39m(classificationCompare\u001b[38;5;241m.\u001b[39mindex)]\u001b[38;5;241m=\u001b[39m[dataset,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moriginal\u001b[39m\u001b[38;5;124m\"\u001b[39m, accuracy,F1,precision,recall,AUC]\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;66;03m#classification on concatinated dataset\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m     accuracy,F1,precision,recall,AUC \u001b[38;5;241m=\u001b[39m  \u001b[43mepochsScore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m     classificationCompare\u001b[38;5;241m.\u001b[39mloc[\u001b[38;5;28mlen\u001b[39m(classificationCompare\u001b[38;5;241m.\u001b[39mindex)]\u001b[38;5;241m=\u001b[39m[dataset,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconcatinated\u001b[39m\u001b[38;5;124m\"\u001b[39m, accuracy,F1,precision,recall,AUC]\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m classificationCompare\n",
      "Cell \u001b[1;32mIn [53], line 10\u001b[0m, in \u001b[0;36mepochsScore\u001b[1;34m(dataset, k)\u001b[0m\n\u001b[0;32m      8\u001b[0m test\u001b[38;5;241m=\u001b[39mpd\u001b[38;5;241m.\u001b[39mread_csv(path\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_test_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(x)\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m'\u001b[39m,sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m'\u001b[39m, index_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m      9\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdatasets/\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39mdataset)\n\u001b[1;32m---> 10\u001b[0m train,test\u001b[38;5;241m=\u001b[39m \u001b[43mcon\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m lda_con \u001b[38;5;241m=\u001b[39m LinearDiscriminantAnalysis()        \n\u001b[0;32m     13\u001b[0m X_train_con \u001b[38;5;241m=\u001b[39m train\u001b[38;5;241m.\u001b[39mdrop(train\u001b[38;5;241m.\u001b[39mcolumns[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[1;32mIn [50], line 4\u001b[0m, in \u001b[0;36mcon\u001b[1;34m(dftrain, dftest, df)\u001b[0m\n\u001b[0;32m      2\u001b[0m minlabel, majlabel\u001b[38;5;241m=\u001b[39m get_labels(dftrain)\n\u001b[0;32m      3\u001b[0m majds \u001b[38;5;241m=\u001b[39m dftrain[dftrain[dftrain\u001b[38;5;241m.\u001b[39mcolumns[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]] \u001b[38;5;241m==\u001b[39m majlabel]\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28mprint\u001b[39m((\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mminlabel\u001b[49m)\u001b[38;5;241m.\u001b[39msum())\n\u001b[0;32m      5\u001b[0m minds \u001b[38;5;241m=\u001b[39m dftrain[dftrain[dftrain\u001b[38;5;241m.\u001b[39mcolumns[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]] \u001b[38;5;241m==\u001b[39m minlabel]\n\u001b[0;32m      8\u001b[0m IRO\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(majds\u001b[38;5;241m.\u001b[39mindex)\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(minds\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\ops\\common.py:69\u001b[0m, in \u001b[0;36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[0;32m     67\u001b[0m other \u001b[38;5;241m=\u001b[39m item_from_zerodim(other)\n\u001b[1;32m---> 69\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\arraylike.py:32\u001b[0m, in \u001b[0;36mOpsMixin.__eq__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;129m@unpack_zerodim_and_defer\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__eq__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__eq__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[1;32m---> 32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cmp_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meq\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py:5502\u001b[0m, in \u001b[0;36mSeries._cmp_method\u001b[1;34m(self, other, op)\u001b[0m\n\u001b[0;32m   5499\u001b[0m rvalues \u001b[38;5;241m=\u001b[39m extract_array(other, extract_numpy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, extract_range\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   5501\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m np\u001b[38;5;241m.\u001b[39merrstate(\u001b[38;5;28mall\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m-> 5502\u001b[0m     res_values \u001b[38;5;241m=\u001b[39m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcomparison_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   5504\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_construct_result(res_values, name\u001b[38;5;241m=\u001b[39mres_name)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\ops\\array_ops.py:261\u001b[0m, in \u001b[0;36mcomparison_op\u001b[1;34m(left, right, op)\u001b[0m\n\u001b[0;32m    256\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(rvalues, (np\u001b[38;5;241m.\u001b[39mndarray, ABCExtensionArray)):\n\u001b[0;32m    257\u001b[0m     \u001b[38;5;66;03m# TODO: make this treatment consistent across ops and classes.\u001b[39;00m\n\u001b[0;32m    258\u001b[0m     \u001b[38;5;66;03m#  We are not catching all listlikes here (e.g. frozenset, tuple)\u001b[39;00m\n\u001b[0;32m    259\u001b[0m     \u001b[38;5;66;03m#  The ambiguous case is object-dtype.  See GH#27803\u001b[39;00m\n\u001b[0;32m    260\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(lvalues) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(rvalues):\n\u001b[1;32m--> 261\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    262\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLengths must match to compare\u001b[39m\u001b[38;5;124m\"\u001b[39m, lvalues\u001b[38;5;241m.\u001b[39mshape, rvalues\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m    263\u001b[0m         )\n\u001b[0;32m    265\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_extension_dispatch(lvalues, rvalues) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m    266\u001b[0m     (\u001b[38;5;28misinstance\u001b[39m(rvalues, (Timedelta, BaseOffset, Timestamp)) \u001b[38;5;129;01mor\u001b[39;00m right \u001b[38;5;129;01mis\u001b[39;00m NaT)\n\u001b[0;32m    267\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_object_dtype(lvalues\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m    268\u001b[0m ):\n\u001b[0;32m    269\u001b[0m     \u001b[38;5;66;03m# Call the method on lvalues\u001b[39;00m\n\u001b[0;32m    270\u001b[0m     res_values \u001b[38;5;241m=\u001b[39m op(lvalues, rvalues)\n",
      "\u001b[1;31mValueError\u001b[0m: ('Lengths must match to compare', (244,), (0,))"
     ]
    }
   ],
   "source": [
    "classificationCompare = complda(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cc2455",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "639bee90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Version</th>\n",
       "      <th>Accuracy score %</th>\n",
       "      <th>F1 Score %</th>\n",
       "      <th>precision score %</th>\n",
       "      <th>recall Score %</th>\n",
       "      <th>AUC Score %</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pima-indians-diabetes.csv</td>\n",
       "      <td>original</td>\n",
       "      <td>81.170</td>\n",
       "      <td>80.030</td>\n",
       "      <td>84.150</td>\n",
       "      <td>81.170</td>\n",
       "      <td>77.800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pima-indians-diabetes.csv</td>\n",
       "      <td>concatinated</td>\n",
       "      <td>74.748</td>\n",
       "      <td>75.198</td>\n",
       "      <td>76.698</td>\n",
       "      <td>74.748</td>\n",
       "      <td>74.972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>glass0.csv</td>\n",
       "      <td>original</td>\n",
       "      <td>65.120</td>\n",
       "      <td>60.560</td>\n",
       "      <td>72.550</td>\n",
       "      <td>65.120</td>\n",
       "      <td>62.830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>glass0.csv</td>\n",
       "      <td>concatinated</td>\n",
       "      <td>72.070</td>\n",
       "      <td>80.198</td>\n",
       "      <td>94.310</td>\n",
       "      <td>72.070</td>\n",
       "      <td>13.774</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Dataset       Version  Accuracy score %  F1 Score %  \\\n",
       "0  pima-indians-diabetes.csv      original            81.170      80.030   \n",
       "1  pima-indians-diabetes.csv  concatinated            74.748      75.198   \n",
       "2                 glass0.csv      original            65.120      60.560   \n",
       "3                 glass0.csv  concatinated            72.070      80.198   \n",
       "\n",
       "   precision score %  recall Score %  AUC Score %  \n",
       "0             84.150          81.170       77.800  \n",
       "1             76.698          74.748       74.972  \n",
       "2             72.550          65.120       62.830  \n",
       "3             94.310          72.070       13.774  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classificationCompare.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb57e0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
